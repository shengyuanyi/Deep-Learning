# 输入库
import requests
import csv
import pandas as pd
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

# 建立函数 - 合理访问网站
def get_one_page(url):
    try:
        headers = {'User-Agent':'Mozilla/5.0(Macintosh;Intel Mac OS X 10_13_3)'  
        + 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'}
        
        # 读取网站内容
        html = requests.get(url,headers = headers)
        
        if html.status_code == 200:
            return html.text
        return None
     
     except RequestException:
        return None
        
        
# 建立函数 - BeautifulSoup解析网站内容

result_all = []   #用于储存信息
def parse_one_page(html):
    soup = BeautifulSoup(html, 'lxml')
    # 对其中job-primary部分进行提取
    companies = soup.find_all('div','job-primary',True)
    
    for com in companies:
        res = parse_one_company(com)
        result_all.append(res)
        
        
 # 建立函数 - 解析后的内容进行提取
 
 def parse_one_conpany(comp):
      result = []
      
     # 公司名称
     company_soup = comp.find('div', class_ = 'info-company')
     com_desc = company_soup.find('p').text
     
     # 工作名称、工资、要求等
     primary_soup = comp.find('div', class = 'info-primary')
     job_name = primary_soup.find('div').text
     salary = primary_soup.find('span').text
     requirement = primary_soup.find('p').text
     
     # 建立数据表格
     result.append(com_desc)
     result.append(job_name)
     result.append(salary)
     result.append(requirement)
     
     return result
     
# 建立函数 - 提供网站信息，链接上述三个函数

def parse_all_page(num, offset):
    url1 = 'https://www.zhipin.com/c100010000/?query=数据分析师&page='   
    + str(offset)+ '&ka=page-' +str(offset)
    
    urldick = {'1', url1}
    
    html = get_one_page(urldick[str(num)])
    parse_one_page(html)

# 对于数据分析师工作开始从第一页到第二十也开始获取信息

if __name__ == '__main__':
    for i in range(1,20):
        parse_all_page(i)
        
    file = pd.DataFrame(result_all, columns = ['公司名称','岗位','薪水','其他'])
    
    # encoding='utf_8_sig'解决保存到CSV文件后的乱码问题
    file.to_csv('Zhipin1.csv', mode='a', index = True, encoding = 'utf_8_sig')
    
    
# 导入数据

import numpy as np
import pandas as pd
import re

data_df = pd.read_csv('Zhipin1.csv', usecols = [1,2,3,4])

data_df.head(10)
 
 
# 数据重新编辑
# 公司信息组合形式： 性质+人数+‘人以上’
# 薪水组合形式：     X k- XX k
# 其他组合形式：     城市 区县 地址 年限 学历

others = list(data_df['其他'])
# 从中提取学历信息，该特征可以作为空格后最后的字
# 从中提取经验信息，该特征可以全部用数字提取

leng = len(others)
for i in range(leng):
    others[i] = other[i].replace('经验不限','0-n ')
    others[i] = other[i].replace('应届生','0-0 ')
    others[i] = other[i].replace('年',' ')

# 对经验、学历、城市进行归类
city, exp, edu = [], [], []
for s in others:
    temp = s.split('')
    city.append(temp[0])
    
    edu.append(temp[-1])
    
    exps = s[-6:-2]
    exp.append(exps)
    
data_df['地区'] = city
data_df['学历'] = city
data_df['经验'] = city
    
# 对薪水进行重新编辑
salary_low, salary_high, salary_all = [], [], []
salary = list[data_df['薪水']]
leng2 = len(salary)
for w in leng2：
    salary[w] = salary[w].replace('k-',' ')
    salary[w] = salary[w].replace('k',' ')
    
    temp = salary[w].split('')
    salary_low.append(temp[0])
    salary_high.append(temp[1])
    
    data_df['最低工资'] = salary_low
    data_df['最高工资'] = salary_high
    
    
data_df.to_excel('Zhipin_summary.xlsx', index = True, encoding = 'utf_8_sig')
 
