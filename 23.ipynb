{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_page(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) ' + \n",
    "            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'\n",
    "        }\n",
    "        html = requests.get(url, headers=headers)\n",
    "        if html.status_code == 200:\n",
    "            return html.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_all = [] # 用于存储样本\n",
    " \n",
    "def parse_one_page(html):\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    companies = soup.find_all('div', 'job-primary', True)\n",
    "    \n",
    "    for com in companies:\n",
    "        res = parse_one_company(com)\n",
    "        result_all.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_company(comp):\n",
    "    result = []\n",
    "    company_soup = comp.find('div', class_='info-company')\n",
    "    com_desc = company_soup.find('p').text\n",
    " \n",
    "    primary_soup = comp.find('div', class_='info-primary')\n",
    "    job_name = primary_soup.find('div').text\n",
    "    salary = primary_soup.find('span').text\n",
    "    requirement = primary_soup.find('p').text\n",
    " \n",
    "    result.append(com_desc)\n",
    "    result.append(job_name)\n",
    "    result.append(salary)\n",
    "    result.append(requirement)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_page(num, offset):\n",
    "    url1 = 'https://www.zhipin.com/c101210100/?query=数据分析师&page='+str(offset)+'&ka=page-'+str(offset)\n",
    "    #url1 = 'https://www.zhipin.com/c101280100/h_101280100/?query=数据分析师&page='+str(offset)+'&ka=page-'+str(offset) # 广州\n",
    "    #url2 = 'https://www.zhipin.com/c101280600/h_101280600/?query=数据分析师&page='+str(offset)+'&ka=page-'+str(offset) # 深圳\n",
    "    #url3 = 'https://www.zhipin.com/c101010100/h_101010100/?query=数据分析师&page='+str(offset)+'&ka=page-'+str(offset) # 北京\n",
    "    #url4 = 'https://www.zhipin.com/c101020100/h_101020100/?query=数据分析师&page='+str(offset)+'&ka=page-'+str(offset) # 上海\n",
    "    urldict = {'1':url1}\n",
    " \n",
    "    html = get_one_page(urldict[str(num)])\n",
    "    parse_one_page(html)\n",
    "\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    for j in range(1, 2):\n",
    "        for i in range(1,20):\n",
    "            parse_all_page(j, i)\n",
    "    file = pd.DataFrame(result_all, columns=['公司信息', '岗位', '薪水', '其他'])\n",
    "    # encoding='utf_8_sig解决保存到CSV文件后显示乱码问题\n",
    "    file.to_csv('BosszhipingHZ.csv', mode='a', index=True, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "import re\n",
    " \n",
    "# 导入数据，第一列的序号不导入\n",
    "data_df = pd.read_csv('Bosszhiping1.csv', usecols=[1,2,3,4])\n",
    " \n",
    "# 查看数据前5行信息，简单了解下数据\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = list(data_df['其他'])\n",
    "# 对edu列表进一步解析，得到“工作经验”和“学历”特征\n",
    "leng = len(others)\n",
    "for i in range(leng):\n",
    "    others[i] = others[i].replace('应届生','0-0 ')\n",
    "    others[i] = others[i].replace('经验不限', '0-n ')\n",
    "    others[i] = others[i].replace('年', ' ') \n",
    "    others[i] = others[i].replace('区 ', '区')\n",
    "    others[i] = others[i].replace('10', '9')    \n",
    "\n",
    "# 工作经验和学历没有空格隔开，我们暂时将两者放到edu列表中\n",
    "city, exp, edu= [], [], []\n",
    "leng = len(others)\n",
    "for s in others:\n",
    "    temp = s.split(' ')\n",
    "    city.append(temp[0])\n",
    "    edus=s[-2:]\n",
    "    edu.append(edus)\n",
    "    exps=s[-6:-2]\n",
    "    exp.append(exps)\n",
    "data_df['地区']=city\n",
    "data_df['学历']=edu\n",
    "data_df['经验']=exp\n",
    "\n",
    "# 对edu列表进一步解析，得到“工作经验”和“学历”特征\n",
    "exp=data_df['经验']\n",
    "leng = len(exp)\n",
    "for i in range(leng):\n",
    "    exp[i] = exp[i].replace('9', '10')\n",
    "    exp[i] = exp[i].replace('以内', '')  \n",
    "salary = list(data_df['薪水'])\n",
    "salary_low,salary_up,salary_all=[],[],[]\n",
    "\n",
    "for s in salary:\n",
    "    t = s.split('k')\n",
    "    salary_all.append(t)\n",
    "data_df['薪水1']=salary_all    \n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文件\n",
    "data_df.to_excel('bossHZ.xls', index=True, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
