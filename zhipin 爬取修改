# 输入库
import requests
import csv
import pandas as pd
from bs4 import BeautifulSoup
from requests.exceptions import RequestException

# 建立函数 - 合理访问网站
def get_one_page(url):
    try:
        headers = {'User-Agent':'Mozilla/5.0(Macintosh;Intel Mac OS X 10_13_3)'  
        + 'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'}
        
        # 读取网站内容
        html = requests.get(url,headers = headers)
        
        if html.status_code == 200:
            return html.text
        return None
     
     except RequestException:
        return None
        
        
# 建立函数 - BeautifulSoup解析网站内容

result_all = []   #用于储存信息
def parse_one_page(html):
    soup = BeautifulSoup(html, 'lxml')
    # 对其中job-primary部分进行提取
    companies = soup.find_all('div','job-primary',True)
    
    for com in companies:
        res = parse_one_company(com)
        result_all.append(res)
        
        
 # 建立函数 - 解析后的内容进行提取
 
 def parse_one_conpany(comp):
      result = []
      
     # 公司名称
     company_soup = comp.find('div', class_ = 'info-company')
     com_desc = company_soup.find('p').text
     
     # 工作名称、工资、要求等
     primary_soup = comp.find('div', class = 'info-primary')
     job_name = primary_soup.find('div').text
     salary = primary_soup.find('span').text
     requirement = primary_soup.find('p').text
     
     # 建立数据表格
     result.append(com_desc)
     result.append(job_name)
     result.append(salary)
     result.append(requirement)
     
     return result
     
# 建立函数 - 提供网站信息，链接上述三个函数

def parse_all_page(num, offset):
    url1 = 'https://www.zhipin.com/c100010000/?query=数据分析师&page='   
    + str(offset)+ '&ka=page-' +str(offset)
    
    urldick = {'1', url1}
    
    html = get_one_page(urldick[str(num)])
    parse_one_page(html)

# 对于数据分析师工作开始从第一页到第二十也开始获取信息

if __name__ == '__main__':
    for i in range(1,20):
        parse_all_page(i)
        
    file = pd.DataFrame(result_all, columns = ['公司名称','岗位','薪水','其他'])
    
    # encoding='utf_8_sig'解决保存到CSV文件后的乱码问题
    file.to_csv('Zhipin1.csv', mode='a', index = True, encoding = 'utf_8_sig')
    
    
    
    

    


 
